"""
æœ¬è„šæœ¬ç”¨äºå®ç°ä»¥ä¸‹åŠŸèƒ½ï¼š

1ï¸âƒ£ è‡ªåŠ¨æŠ“å–æŸåªè‚¡ç¥¨åœ¨è¿‡å» N å¤©å†…çš„ç›¸å…³æ–°é—»ï¼ˆæ”¯æŒæ¥æºï¼šFinnhub ç­‰ï¼‰ï¼›
2ï¸âƒ£ åˆ©ç”¨ FinBERT æ¨¡å‹å¯¹æ¯æ¡æ–°é—»æ ‡é¢˜è¿›è¡Œæƒ…ç»ªåˆ†æï¼Œåˆ¤æ–­ä¸º positive / neutral / negativeï¼Œå¹¶è®¡ç®—ç½®ä¿¡åº¦ï¼›
3ï¸âƒ£ æ ¹æ®ä¸åŒæ–°é—»æ¥æºçš„å¯ä¿¡åº¦è®¾å®šâ€œåŠ æƒå€¼â€ï¼Œç”ŸæˆåŠ æƒæƒ…ç»ªå¾—åˆ†ï¼ˆweighted_scoreï¼‰ï¼›
4ï¸âƒ£ å¯è§†åŒ–æ¯æ—¥å¹³å‡åŠ æƒæƒ…ç»ªå¾—åˆ†ï¼Œç”¨äºè§‚å¯Ÿå¸‚åœºæƒ…ç»ªè¶‹åŠ¿ã€‚

ğŸ“Œ æ–°é—»æ¥æºæƒé‡è®¾å®šè¯´æ˜ï¼ˆsource_weightsï¼‰ï¼š
    ä¸ºäº†è®©æƒ…ç»ªåˆ†æç»“æœæ›´è´´è¿‘å¸‚åœºå®é™…æƒ…å†µï¼Œæˆ‘ä»¬ä¸ºä¸åŒçš„æ–°é—»æ¥æºèµ‹äºˆäº†ä¸åŒçš„æƒé‡ï¼Œä¾æ®å¦‚ä¸‹ï¼š
    - ğŸ§  æ›´ä¸“ä¸šçš„é‡‘èæ–°é—»åª’ä½“ï¼ˆå¦‚ WSJ, Bloomberg, SeekingAlphaï¼‰èµ‹äºˆæ›´é«˜æƒé‡ï¼ˆ1.1ï½1.2ï¼‰ï¼›
    - ğŸ“£ é€šç”¨æ–°é—»ç½‘ç«™æˆ–è‡ªåŠ¨èšåˆç±»åª’ä½“ï¼ˆå¦‚ Yahoo, Foolï¼‰èµ‹äºˆä¸­ç­‰æƒé‡ï¼ˆçº¦ 1.0ï¼‰ï¼›
    - ğŸ“± å†…å®¹è¾ƒçŸ­ã€ç®€ç•¥æˆ–åæ ‡é¢˜å…šçš„åª’ä½“ï¼ˆå¦‚ Benzingaï¼‰èµ‹äºˆè¾ƒä½æƒé‡ï¼ˆçº¦ 0.8ï¼‰ã€‚

    æƒé‡èŒƒå›´å¤§è‡´ä¸º 0.8 åˆ° 1.2ï¼Œç”¨äºè°ƒèŠ‚æƒ…ç»ªæ‰“åˆ†çš„å½±å“åŠ›ï¼Œå¢å¼ºé‡è¦æ¥æºçš„åˆ†ææƒé‡ã€‚

ä½¿ç”¨æ–¹å¼ï¼š
è¿è¡Œè„šæœ¬åæŒ‰æç¤ºè¾“å…¥è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ AAPLï¼‰å’Œåˆ†æå¤©æ•°ï¼ˆå»ºè®®ä¸è¶…è¿‡ 7 å¤©ï¼‰ï¼Œç¨‹åºä¼šè‡ªåŠ¨æ‰§è¡Œåˆ†ææµç¨‹ã€‚

Note: å…è´¹ç‰ˆ Finnhub API ä»…æ”¯æŒæŠ“å–è¿‡å» 7 å¤©å†…çš„æ–°é—»æ•°æ®ã€‚
"""

import os
import pandas as pd
import time
import matplotlib.pyplot as plt
from transformers import BertTokenizer, BertForSequenceClassification
import torch
from tqdm import tqdm
from fetch_news.news_api import get_news

# åŠ è½½ FinBERT æ¨¡å‹
tokenizer = BertTokenizer.from_pretrained('ProsusAI/finbert')
model = BertForSequenceClassification.from_pretrained('ProsusAI/finbert')

# ========== æ­¥éª¤1ï¼šæŠ“å–æ–°é—» ==========
def fetch_news(ticker, days, source='finnhub'):
    if days > 7 and source == 'finnhub':
        print("âš ï¸ å…è´¹ç‰ˆ Finnhub API åªèƒ½æŠ“å–è¿‡å» 7 å¤©çš„æ–°é—»ï¼Œè¯·è°ƒæ•´å¤©æ•°æˆ–æ›´æ¢ API æ¥æº")
        return None

    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    save_path = os.path.join(project_root, "data", "raw")
    os.makedirs(save_path, exist_ok=True)

    try:
        df = get_news(ticker, days, source=source)
        if df is not None and not df.empty:
            output_file = f"{ticker}_news_{days}d_{source}.csv"
            df.to_csv(os.path.join(save_path, output_file), index=False)
            print(f"âœ… News for {ticker} saved to {output_file}")
            return os.path.join(save_path, output_file)
        else:
            print(f"âš ï¸ No news found for {ticker}")
            return None
    except Exception as e:
        print(f"âŒ Error fetching news: {e}")
        return None

# ========== æ­¥éª¤2ï¼šæƒ…ç»ªåˆ†æ ==========
def classify_sentiment(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    probs = torch.nn.functional.softmax(outputs.logits, dim=1)
    confidence, prediction = torch.max(probs, dim=1)
    label_map = {0: 'negative', 1: 'neutral', 2: 'positive'}
    return label_map[prediction.item()], round(confidence.item(), 4)

def analyze_sentiment(filepath, ticker, days, source):
    df = pd.read_csv(filepath)
    if 'headline' not in df.columns:
        raise ValueError("Missing 'headline' column in news file")

    sentiments = []
    confidences = []
    print("ğŸ” Running FinBERT sentiment analysis...")

    for text in tqdm(df['headline']):
        label, conf = classify_sentiment(str(text))
        sentiments.append(label)
        confidences.append(conf)

    df['sentiment'] = sentiments
    df['confidence'] = confidences
    df['date'] = pd.to_datetime(df['datetime']).dt.date
    df['score'] = df['sentiment'].map({'positive': 1, 'neutral': 0, 'negative': -1})

    # åŠ æƒè®¾ç½®
    source_weights = {
        'seekingalpha': 1.2,
        'marketwatch': 1.0,
        'bloomberg': 1.1,
        'cnbc': 0.9,
        'wsj': 1.2,
        'benzinga': 0.8,
        'yahoo': 1.0,
        'investorplace': 0.85,
        'reuters': 1.1,
        'fool': 0.95,
        'default': 1.0
    }

    def get_weight(source_name):
        return source_weights.get(str(source_name).lower(), source_weights['default'])

    df['source_weight'] = df['source'].apply(get_weight)
    df['weighted_score'] = df['score'] * df['source_weight']

    # ä¿å­˜å¤„ç†ç»“æœ
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    processed_path = os.path.join(project_root, "data", "processed")
    os.makedirs(processed_path, exist_ok=True)
    output_file = os.path.join(processed_path, f"{ticker}_news_{days}d_{source}_sentiment.csv")
    df.to_csv(output_file, index=False)
    print(f"âœ… Processed data saved to: {output_file}")

    return df

# ========== æ­¥éª¤3ï¼šç”»æŠ˜çº¿å›¾ ==========
def plot_sentiment_trend(df, ticker):
    if 'weighted_score' not in df.columns or df['weighted_score'].dropna().empty:
        print("âš ï¸ åŠ æƒå¾—åˆ†ä¸ºç©ºï¼Œè¯·æ£€æŸ¥ weighted_score æ˜¯å¦æ­£ç¡®è®¡ç®—")
        return

    daily_weighted_avg = df.groupby('date')['weighted_score'].mean()

    if daily_weighted_avg.empty:
        print("âš ï¸ æ— æœ‰æ•ˆçš„åŠ æƒæ•°æ®ç”¨äºç»˜å›¾")
        return

    plt.figure(figsize=(10, 5))
    daily_weighted_avg.plot(kind='line', marker='o')
    plt.title(f"{ticker} - Daily Weighted Sentiment Score")
    plt.ylabel("Weighted Sentiment Score (-1 to 1)")
    plt.xlabel("Date")
    plt.grid(True)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()


# ========== ä¸»æµç¨‹ ==========
def run_pipeline():
    ticker = input("è¯·è¾“å…¥è‚¡ç¥¨ä»£ç  (å¦‚ AAPL): ").upper()
    if not ticker.isalpha() or len(ticker) > 5:
        print("âŒ è‚¡ç¥¨ä»£ç æ ¼å¼é”™è¯¯")
        return

    try:
        days = int(input("è¯·è¾“å…¥åˆ†æå¤©æ•° (å»ºè®®å¡« 7): "))
    except:
        print("âŒ å¤©æ•°è¾“å…¥é”™è¯¯")
        return

    source = 'finnhub'
    news_file = fetch_news(ticker, days, source)
    if news_file:
        df = analyze_sentiment(news_file, ticker, days, source)
        plot_sentiment_trend(df, ticker)

if __name__ == "__main__":
    run_pipeline()
